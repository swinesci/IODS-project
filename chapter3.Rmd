# Excersize 3

```{r}
alc <- read.csv("Z:\\IODS-project\\data\\alc.csv", sep = ",")
```

# access the tidyverse libraries dplyr and ggplot2
```{r}
library(dplyr)
```

```{r}
library(ggplot2)
```
# produce summary statistics by group
```{r}
alc %>% group_by(sex) %>% summarise(count = n())
```

library(ggplot2)

# initialize a plot of high_use and G3
```{r}
g1 <- ggplot(alc, aes(x = high_use, y = G3, col = sex))
```

# define the plot as a boxplot and draw it
```{r}
g1 + geom_boxplot() + ylab("grade")
```

# initialise a plot of high_use and absences
```{r}
g2 <- ggplot(alc, aes(x = high_use, y = absences, col = sex))
```

# define the plot as a boxplot and draw it
```{r}
g2 + geom_boxplot() + ggtitle("Student absences by alcohol consumption and sex")
```



# find the model with glm()
```{r}
m <- glm(high_use ~ failures + absences + sex, data = alc, family = "binomial")
```

# print out a summary of the model
```{r}
summary(m)
```

# print out the coefficients of the model
```{r}
coef(m)
```
# alc and dlyr are available 

# find the model with glm()
```{r}
m <- glm(high_use ~ failures + absences + sex, data = alc, family = "binomial")
```

# compute odds ratios (OR)
```{r}
OR <- coef(m) %>% exp
```
# compute confidence intervals (CI)
```{r}
CI <- confint(m) %>% exp
```

# print out the odds ratios with their confidence intervals
```{r}
cbind(OR, CI)
```

# fit the model
```{r}
m <- glm(high_use ~ failures + absences + sex, data = alc, family = "binomial")
```

# predict() the probability of high_use
```{r}
probabilities <- predict(m, type = "response")
```

# add the predicted probabilities to 'alc'
```{r}
alc <- mutate(alc, probability = probabilities)
```

# use the probabilities to make a prediction of high_use
```{r}
alc <- mutate(alc, prediction = probability > 0.5)
```

# see the last ten original classes, predicted probabilities, and class predictions
```{r}
select(alc, failures, absences, sex, high_use, probability, prediction) %>% tail(10)
```

# tabulate the target variable versus the predictions
```{r}
table(high_use = alc$high_use, prediction = alc$prediction)
```

# access dplyr and ggplot2
```{r}
library(dplyr); library(ggplot2)
```

# initialize a plot of 'high_use' versus 'probability' in 'alc'
```{r}
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))
```

# define the geom as points and draw the plot
```{r}
g + geom_point()
```

# tabulate the target variable versus the predictions
```{r}
table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table %>% addmargins
```

# the logistic regression model m and dataset alc with predictions are available

# define a loss function (average prediction error)
```{r}
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
```

# call loss_func to compute the average number of wrong predictions in the (training) data
```{r}
loss_func(class = alc$high_use, prob = alc$probability)
```
# the logistic regression model m and dataset alc (with predictions) are available

# define a loss function (average prediction error)
```{r}
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
```

# compute the average number of wrong predictions in the (training) data
```{r}
loss_func(class = alc$high_use, prob = alc$probability)
```


# K-fold cross-validation
```{r}
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)
```

# average number of wrong predictions in the cross validation
```{r}
cv$delta[1]
```

